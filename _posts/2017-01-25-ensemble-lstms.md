---
layout: post
section-type: post
category: project
date: 2017-01-25T00:00:00Z
tagline: Ensemble Methods for LSTMs 
tags:
- machine learning
- long short-term memory networks 
- deep learning 
- language modeling
- adaboost
- boosting 
title: Deep Ensemble Methods for Language Modeling 
--- 

Abstract:
In this paper, we present two new ensemble methods each with two variants for Recurrent Neural Networks (RNNs) which have Long Short-Term Memory (LSTM) units. We propose two new methods: (1) AdaBoost Inspired Mini- Batch Sampling (ABIMBS) and (2) AdaBoost Inspired Sentence Sampling (ABISS) ensemble methods for the language modeling task. ABIMBS has a forward and backward variant and ABISS has a standard deviation and square root variant. We show that all four of these methods applied to both non-dropout and dropout LSTM architectures for language modeling achieve lower perplexity than its current state-of-the- art independently trained and uniformly averaged ensemble counterparts.


